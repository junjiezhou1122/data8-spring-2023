{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datascience import *\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plots\n",
    "plots.style.use('fivethirtyeight')\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 21"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Students Flipping Fair Coins Conclude Coin is Unfiar!\n",
    "\n",
    "Suppose there are 2000 students in Data 8 and each student:\n",
    "* is given a fair coin by the instructor but _they are not told that it is a fair coin_\n",
    "* **collects data** by flipping the coin **100 times** and counts the number of times it lands Heads\n",
    "* runs a **hypothesis test**:\n",
    "    * **Null Hypothesis:** They were givne a fair coin and the number of heads observed see is due to chance.\n",
    "    * **Alternative Hypothesis:** The coin is biased and so the number of heads they observed is not due to chance alone.\n",
    "    * **Test Statistic:** abs(num_heads - 50)\n",
    "* runs *1000* simulations of flipping a fiar coin 100 times (using Python) \n",
    "* reports their *p-value* and rejects the null hypothesis if their p-value is less than 0.5\n",
    "\n",
    "We know that we gave all of them fair coints.  How often will they incorrectly reject the null hypothesis? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Could you write code to simulate the process of one student running this hypothesis test? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_one_hypothesis_test():\n",
    "    num_coin_flips = 100\n",
    "    # Student Collects Data by actually flipping the coin (we simualte here)\n",
    "    # its fair coin but we didn't tell the studnet\n",
    "    obs_flips = np.random.choice([\"H\", \"T\"], num_coin_flips) \n",
    "    \n",
    "    # Define the test statistic\n",
    "    def test_statistic(flips_dataset):\n",
    "        num_heads = sum(flips_dataset == \"H\")\n",
    "        return np.abs(num_heads - num_coin_flips/2)\n",
    "    \n",
    "    # Compute the observed value of the statistic on our actual data\n",
    "    obs_statistic = test_statistic(obs_flips)\n",
    "    \n",
    "    # Define a function to simulate the statistic under the null hypothesis\n",
    "    def simulate_one_statistic():\n",
    "        sim_flips = np.random.choice([\"H\", \"T\"], num_coin_flips) \n",
    "        sim_statistic = test_statistics(sim_flips)\n",
    "        return sim_statistic\n",
    "        \n",
    "    # Hypothesis Test: Simulate from Null hypothesis\n",
    "    simulated_statistics = make_array()\n",
    "    for i in np.arange(1000):  # Simulate 1000 trials  \n",
    "        simulated_statistics = np.append(simulated_statistics, simulate_one_statistic() )\n",
    "    \n",
    "    # Compute the P Value\n",
    "    p_value = sum(simulated_statistics >= obs_statistic) / len(simulated_statistics) \n",
    "    \n",
    "    return p_value # The p-value that the simulated student got"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution below...\n",
    "\n",
    "<br><br><br><br>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Simulating the Simulation \n",
    "\n",
    "In the following we will use simulation to simulate the students running a simulation.  Very meta!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_one_hypothesis_test():\n",
    "    num_coin_flips = 100\n",
    "    num_simulations = 1000\n",
    "    # Student Collects Data by actually flipping the coin (we simualte here)\n",
    "    # its fair coin but we didn't tell the studnet\n",
    "    obs_flips = np.random.choice([\"H\", \"T\"], num_coin_flips) \n",
    "    \n",
    "    # Define the test statistic\n",
    "    def test_statistic(flips_dataset):\n",
    "        num_heads = sum(flips_dataset == \"H\")\n",
    "        return np.abs(num_heads - num_coin_flips/2)\n",
    "    \n",
    "    # Compute the observed value of the statistic on our actual data\n",
    "    obs_statistic = test_statistic(obs_flips)\n",
    "    \n",
    "    # Define a function to simulate the statistic under the null hypothesis\n",
    "    def simulate_one_statistic():\n",
    "        sim_flips = np.random.choice([\"H\", \"T\"], num_coin_flips)\n",
    "        sim_statistic = test_statistic(sim_flips)\n",
    "        return sim_statistic\n",
    "        \n",
    "    # Hypothesis Test: Simulate from Null hypothesis\n",
    "    simulated_statistics = make_array()\n",
    "    for i in np.arange(num_simulations):  # Simulate 1000 trials  \n",
    "        simulated_statistics = np.append(simulated_statistics, simulate_one_statistic())\n",
    "    \n",
    "    # Compute the P Value\n",
    "    p_value = sum(simulated_statistics >= obs_statistic)/len(simulated_statistics)\n",
    "    \n",
    "    return p_value\n",
    "    \n",
    "\n",
    "\n",
    "simulate_one_hypothesis_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulate the entire class running the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data8_students = make_array()\n",
    "for i in np.arange(2000):\n",
    "    all_data8_students = np.append(all_data8_students, simulate_one_hypothesis_test())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You would seldom do this in practice, but here we can visualize the distribution of p-values that all the students in the class get. Some will conclude that they have an unfair coin.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tbl = Table().with_column(\"P Values\", all_data8_students)\n",
    "tbl.hist(\"P Values\", bins=np.arange(0,1,0.01), right_end=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(all_data8_students <= 0.05), \" would falsely reject the null hypothesis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "\n",
    "---\n",
    "\n",
    "## Super Soda Co and the Case of Bad Taste\n",
    "\n",
    "Manufacturers of Super Soda run a taste test and 91 out of 200 tasters prefer Super Soda over its rival.  The boss is upset!  He asks:\n",
    "\n",
    "    Do fewer people prefer Super Soda, or is this just chance?\n",
    "    \n",
    "You run a hypothesis test:\n",
    "\n",
    "* **Null Hypothesis:** Equal proportions of the population prefer Super Soda as Rival and any variability is due to chance.\n",
    "* **Alternative Hypothesis:** Fewer people in the population prefer Super Soda than its Rival.\n",
    "* **Test Statistic:** Number of people who prefer Super Soda\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You pick a **p-value cutoff of 0.05**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_statistic = 91"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulating the test_statistic from the null hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def simulate_one_count(sample_size):\n",
    "    simulated_data = np.random.choice(['Super', 'Rival'], sample_size)\n",
    "    simulated_statistic = np.count_nonzero(simulated_data == \"Super\")\n",
    "    return simulated_statistic\n",
    "simulate_one_count(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running many simulations of the test statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_simulations = 10_000\n",
    "counts = make_array()\n",
    "for i in np.arange(num_simulations):\n",
    "    counts = np.append(counts, simulate_one_count(200))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the distribution of the test statistic under the null hypothesis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = Table().with_column('Number of Super', counts)\n",
    "trials.hist(right_end=91)\n",
    "plots.ylim(-0.001, 0.055)\n",
    "plots.scatter(91, 0, color='red', s=40, zorder=3)\n",
    "plots.title('Prediction Under the Null');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_value = np.count_nonzero(counts <= 91)/len(counts)\n",
    "print(\"The p-value is\", p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "\n",
    "--- \n",
    "\n",
    "## Changing the number of simulations\n",
    "\n",
    "What happens if we run a different number of simulations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keeping the data fixed, we can re-run the test with a new simulation under the null\n",
    "def simulate_null(num_simulations, sample_size):\n",
    "    counts = make_array()\n",
    "    for i in np.arange(num_simulations):\n",
    "        counts = np.append(counts, simulate_one_count(sample_size))\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keeping the data fixed, we can re-run the test with a new simulation under the null\n",
    "def run_test(num_simulations, sample_size, obs_statistic):\n",
    "    counts = simulate_null(num_simulations, sample_size)\n",
    "    # compute the p value\n",
    "    p_value = np.count_nonzero(counts <= obs_statistic)/len(counts)\n",
    "    return p_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulating the Simulation (Again)\n",
    "\n",
    "We can again run multiple simulations of our simulation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's repeat that 50 times for each number of simulations\n",
    "tests = Table(['simulations', 'p-value for 91'])\n",
    "for k in np.arange(100): # will run the simulation 100 times\n",
    "    for num_sims in [100, 1000, 10000]: \n",
    "        p_value = run_test(num_sims, 200, 91)\n",
    "        tests = tests.with_row([num_sims, p_value])\n",
    "tests.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then visualize the distribution of p-values.  Notice how as we increase the number of simulations the estimate for the p-value concentrates around a single number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For larger numbers of simulations, p-values are more consistent\n",
    "tests.hist(\"p-value for 91\", group='simulations', bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "\n",
    "---\n",
    "\n",
    "## Law of Large Number\n",
    "\n",
    "The reason the p-values concentrate towards the true p-value is that the emprical distribution under the null is better approximates by increasing the number of simulations.  More is better!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = Table().with_columns(\"Number of Super\", simulate_null(100, 200),\n",
    "                           \"Simulation Size\", 100)\n",
    "t2 = Table().with_columns(\"Number of Super\", simulate_null(100_000, 200),\n",
    "                           \"Simulation Size\", 100_000)\n",
    "t1.append(t2).hist(group='Simulation Size', bins=np.arange(70.5, 131, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>\n",
    "\n",
    "---\n",
    "\n",
    "## The Importance of Sample Size\n",
    "\n",
    "Larger samples give us more information about the population and also allow us to test more subtle differences.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that the true proportion of people who prefer Super Soda is 45%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_proportion = 0.45\n",
    "true_distribution = make_array(true_proportion, 1 - true_proportion)\n",
    "true_distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taste tests with 200 people will give varioius numbers of people who prefer Super Soda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 200\n",
    "sample_proportions(sample_size, true_distribution) * sample_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you run a taste test for 200 people, what might you conclude?\n",
    "def run_experiment(num_simulations, sample_size, true_proportion):\n",
    "    # Collect data\n",
    "    true_distribution = make_array(true_proportion, 1 - true_proportion)\n",
    "    taste_test_results = sample_proportions(sample_size, true_distribution) * sample_size\n",
    "    observed_stat_from_this_sample = taste_test_results.item(0)\n",
    "    \n",
    "    # Conduct hypothesis test\n",
    "    p_value = run_test(num_simulations, sample_size, observed_stat_from_this_sample)\n",
    "    return p_value\n",
    "\n",
    "run_experiment(10000, 200, 0.45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try using different values for the true_proportion and sample size.  What happens to as the true proportion gets closer to 0.5?  What happens if we increase the sample size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's imagine running our taste test over and over again to see how often we reject the null\n",
    "true_proportion = 0.45\n",
    "sample_size = 200\n",
    "p_values = make_array()\n",
    "for k in np.arange(100):\n",
    "    p_value = run_experiment(1000, sample_size, true_proportion)\n",
    "    p_values = np.append(p_values, p_value)\n",
    "Table().with_column('P-value', p_values).hist(0, right_end=0.05, bins=np.arange(0,1,0.1))\n",
    "print(\"Percent that correctly reject the null\", 100*np.mean(p_values <= 0.05))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
